{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fc214-2c8f-4833-8382-2fc8f2da7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams, Text, bigrams, BigramCollocationFinder \n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "import collections\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import lxml\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from gensim import utils\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time  \n",
    "import gensim.models\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn import cluster\n",
    "import plotly.express as px\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1778b30-3ac5-4ddf-bc66-7c8c2095c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file\n",
    "list_of_file = {}\n",
    "from os import listdir\n",
    "for file in listdir(\"/home/scc/sergio.zanotto/bin/Identities/Impaqts/discorsi_off/\"):\n",
    "    if file != \".ipynb_checkpoints\":\n",
    "        with open(\"/home/scc/sergio.zanotto/bin/Identities/Impaqts/discorsi_off/\" + file, 'r', encoding='utf-8', errors = \"ignore\") as f:\n",
    "                lines = f.readlines()\n",
    "                list_of_file[file] = lines\n",
    "\n",
    "df = pd.DataFrame(columns=['Filename', 'Text'])\n",
    "for file, lines in list_of_file.items():\n",
    "    text_list = []\n",
    "    fil = re.sub('.txt', '', file)\n",
    "    text_list.append([line.strip() for line in lines])  # Each line is a separate list within the cell\n",
    "    df = df.append({'Filename': fil, 'Text': text_list}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e555b9-b00c-4bb4-ad84-621453403ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculating author when missing\n",
    "gg = []\n",
    "for x in df[\"Filename\"]:\n",
    "    g = False\n",
    "    for y,z in zip(dd[\"NOME\"],dd[\"PARLANTE\"]):\n",
    "        if x==y:\n",
    "            gg.append(z)\n",
    "            g = True\n",
    "    if g == False: \n",
    "        gg.append(\"None\")\n",
    "df[\"author\"] = gg     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dee6f-ad55-4340-a54d-d1635c86b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning the dataset from possible interruptions\n",
    "corpus_speech = []\n",
    "interruption = []\n",
    "for filename, text, author in zip(df[\"Filename\"], df[\"Text\"], df[\"author\"]):\n",
    "    first_interrup = 0\n",
    "    interruptions = 0\n",
    "    text_list = []\n",
    "    current_author = True\n",
    "    mention = False\n",
    "    #print(filename,author)\n",
    "    if author != \"None\":\n",
    "        author = author.upper()\n",
    "        author1 = re.sub(r'[^A-Za-z]', '', author)\n",
    "        for y in text:\n",
    "            for x in y:   \n",
    "                x = re.sub(\"\\/\", '', x)\n",
    "                x = re.sub(\"\\n\", '', x)\n",
    "                x = re.sub(r\"\\+\", '', x)\n",
    "                x = re.sub(\"\\ufeff\", '', x)\n",
    "                words = x.strip().split()\n",
    "                if words:\n",
    "                    first_word = re.sub(r'[^A-Za-z]', '', words[0])\n",
    "                    if (\n",
    "                        (words[0].isupper() and words[0].endswith((\".\", \":\"))) or\n",
    "                        (len(words) > 1 and words[1].isupper() and words[1].endswith((\".\", \":\"))) or\n",
    "                        (len(words) > 2 and words[2].isupper() and words[2].endswith((\".\", \":\")))\n",
    "                    ):\n",
    "                        if words[0].isupper():\n",
    "                            if first_word in author1:\n",
    "                                mention = True\n",
    "                                current_author = True\n",
    "                                author_words = author.split()\n",
    "                                for auth in author_words:\n",
    "                                    x = x.replace(auth, '')  \n",
    "                            else:\n",
    "                                current_author = False \n",
    "                                interruptions += 1\n",
    "                    else:\n",
    "                        if current_author == False:\n",
    "                            if mention == False:\n",
    "                                if (\n",
    "                                    (words[0].isupper() and words[0].endswith((\".\", \":\"))) or\n",
    "                                    (len(words) > 1 and words[1].isupper() and words[1].endswith((\".\", \":\"))) or\n",
    "                                    (len(words) > 2 and words[2].isupper() and words[2].endswith((\".\", \":\")))\n",
    "                                ):\n",
    "                                    current_author = False\n",
    "                                else:\n",
    "                                    current_author = True                            \n",
    "                    if current_author == True:\n",
    "                        text_list.append(x)\n",
    "    #print(text_list ,len(text_list))\n",
    "    interruption.append(interruptions)                        \n",
    "    corpus_speech.append(text_list)\n",
    "\n",
    "df[\"Text\"] = corpus_speech\n",
    "df[\"interruption\"] = interruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fbe8ba-8182-4577-acbb-cc0ad9a552a2",
   "metadata": {},
   "source": [
    "## Calculating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12797269-cf1f-4f5a-8012-7b78e10757b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a zip file for the UD_PROFILING tool\n",
    "df = df[df['Text'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "df['Text'] = df['Text'].apply(lambda x: '\\n'.join(x))\n",
    "\n",
    "# Writing each row's text to a text file named after the 'Filename' column\n",
    "for _, row in df.iterrows():\n",
    "    with open(f\"/home/scc/sergio.zanotto/bin/Identities/Impaqts/Identities/txt/{row['Filename']}.txt\", \"w\") as file:\n",
    "        file.write(row['Text'])\n",
    "# The files will be saved with names from the 'Filename' column\n",
    "import zipfile\n",
    "\n",
    "# Path for the zip file\n",
    "zip_path = 'text_files.zip'\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for _, row in df.iterrows():\n",
    "        # Adding each text file to the zip\n",
    "        text_file_path = f\"/home/scc/sergio.zanotto/bin/Identities/Impaqts/Identities/txt/{row['Filename']}.txt\"\n",
    "        zipf.write(text_file_path, arcname=row['Filename']+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303372d-91d4-4bae-a5e4-a7969c006994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to DataFrame for applying measures\n",
    "\n",
    "dd = pd.read_csv(\"Monitoring.csv\", encoding=\"latin-1\") #not sharable\n",
    "popu = pd.read_csv('populism-anticentrism.csv', encoding='latin-1' #Decadri_Boussalis, 2020\n",
    "AoA = pd.read_csv('AoA_data.csv', encoding='latin-1') # Montefinese et Al., 2019\n",
    "Concrete_it = pd.read_csv('concreteness_it.csv', encoding='latin-1') #Gregori et Al., 2020\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc16c8-c866-4aec-b376-14069df2264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ficing concreteness\n",
    "desired_columns = [\"TARGET\",\"CONC_AVG\"]\n",
    "\n",
    "# Filter the DataFrame to keep only the desired columns\n",
    "Concrete_it = Concrete_it[desired_columns].copy()\n",
    "tt = Concrete_it[\"TARGET\"].values.tolist()\n",
    "concrete_set = set(tt)\n",
    "conrete_value = {}\n",
    "for z in tt:\n",
    "    value = 0\n",
    "    n = 0\n",
    "    for x,y in zip(Concrete_it[\"TARGET\"],Concrete_it[\"CONC_AVG\"]):\n",
    "        if z == x:\n",
    "            n += 1\n",
    "            value += y\n",
    "    conrete_value[z]= value/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e325cc8-3526-45d1-aeb9-ace9e96146ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AoA\n",
    "AoA_words = AoA[\"words\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3838223-54a7-4c51-b21b-c7037cd72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Populist words\n",
    "from numpy import nan\n",
    "\n",
    "# Remove nan values from the list\n",
    "populist_words = [word for word in populist_words if not pd.isnull(word)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f8c83-3976-466f-b348-468fd3b33558",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculate G_Index, Type_Token_Ration, Populist_Words\n",
    "G_index_list = []\n",
    "Type_token_ratio_list = []\n",
    "pop_words_ratio_list = []\n",
    "\n",
    "# Initialize lists to store calculated metrics\n",
    "G_index_list = []\n",
    "Type_token_ratio_list = []\n",
    "pop_words_ratio_list = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for x, y in zip(df[\"Text\"], df[\"interruption\"]):\n",
    "    # Initialize counters and storage for types\n",
    "    types = []\n",
    "    n_sent = len(x) - y\n",
    "    n_word = 0\n",
    "    n_character = 0\n",
    "    n_pop_word = 0\n",
    "    n_word1 = 0\n",
    "    \n",
    "    # Process each sentence in the text\n",
    "    for m in x:\n",
    "        # Clean the text and split into words\n",
    "        m = re.sub('[^\\w\\s]', ' ', m)\n",
    "        m = m.split()\n",
    "        \n",
    "        # Update word counts and type list\n",
    "        n_word += len(m)\n",
    "        for el in m:\n",
    "            n_character += len(el)\n",
    "            types.append(el.lower())\n",
    "            \n",
    "            # Check for populist words and update counts\n",
    "            if el not in italian_stop_words:\n",
    "                n_word1 += 1\n",
    "                for pop_word in populist_words:\n",
    "                    if pop_word in el:\n",
    "                        n_pop_word += 1\n",
    "    \n",
    "    # Calculate G_index using the formula\n",
    "    G_index = 89 + ((300 * n_sent - 10 * n_character) / n_word)\n",
    "    G_index_list.append(G_index)\n",
    "    \n",
    "    # Calculate Type-Token Ratio\n",
    "    Type_token_ratio = len(set(types)) / n_word\n",
    "    Type_token_ratio_list.append(Type_token_ratio)\n",
    "    \n",
    "    # Calculate Populist Words Ratio\n",
    "    pop_words_ratio = n_pop_word / n_word1\n",
    "    pop_words_ratio_list.append(pop_words_ratio)\n",
    "\n",
    "# Assign calculated metrics to DataFrame columns\n",
    "df[\"G_index\"] = G_index_list\n",
    "df[\"Type_token_ratio\"] = Type_token_ratio_list\n",
    "df[\"Populist_words_ratio\"] = pop_words_ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efc568-32e8-40bf-979f-8af8e4f927b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='it') ## using Stanza for PoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2aa5f-7a38-4915-8418-4128438b516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for different measures, but we used only Concreteness\n",
    "pos = [\"VERB\",\"AUX\"]\n",
    "def measures(x1):\n",
    "    x = ' '.join(x1)\n",
    "    x2 = nlp(x)\n",
    "    n_pass = 0\n",
    "    n_verbs = 0\n",
    "    n_sub = 0\n",
    "    new_text = []\n",
    "    concr_lev = 0\n",
    "    n_word_pres = 0\n",
    "    n_word = 0\n",
    "    non_pres_word = 0\n",
    "    for sent in x2.sentences: \n",
    "        for word in sent.words:\n",
    "            if word.text.lower() not in italian_stop_words:  \n",
    "                n_word += 1\n",
    "                new_text.append(word.lemma)\n",
    "                word_is_present = False\n",
    "                for key, value in conrete_value.items():\n",
    "                    if word.lemma.lower() == key:\n",
    "                        word_is_present = True\n",
    "                        n_word_pres += 1\n",
    "                        concr_lev += value    \n",
    "                if word_is_present == False:\n",
    "                    non_pres_word += 1\n",
    "            if word.upos in pos:\n",
    "                n_verbs += 1\n",
    "                if \":pass\" in word.deprel:\n",
    "                    n_pass += 1\n",
    "                if word.feats:\n",
    "                    if \"Mood=Sub\" in word.feats or \"Mood=Cnd\" in word.feats:\n",
    "                        n_sub += 1                    \n",
    "    ratepassvsact = n_pass / n_verbs if n_verbs != 0 else 0  \n",
    "    ratesub = n_sub / n_verbs if n_verbs != 0 else 0\n",
    "    clev = concr_lev/n_word_pres if n_word_pres != 0 else 0\n",
    "    perc_not_pres_word = (non_pres_word/n_word)*100 if n_word != 0 else 0\n",
    "    return ratepassvsact, ratesub, new_text, clev, perc_not_pres_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774fb06-b910-4510-abe9-664c21fc4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AoA and COncreteness\n",
    "ratepassvsact_list= [] #not_used\n",
    "ratesub_list= [] #not_used\n",
    "new_text_list= []\n",
    "dict_emb_list= [] #not_used\n",
    "AoA_list = []\n",
    "concr_list = []\n",
    "i = 0\n",
    "for x in df[\"Text\"]:\n",
    "    i += 1\n",
    "    n_word_inlistlemmas = 0\n",
    "    non_present_word = 0\n",
    "    present_word = 0\n",
    "    AoA_score = 0\n",
    "    ratepassvsact, ratesub, new_text, concr_lev, percentage_non_present_words_in_concrete = measures(x)\n",
    "    \n",
    "    #append elements\n",
    "    ratepassvsact_list.append(ratepassvsact*100)\n",
    "    ratesub_list.append(ratesub*100)\n",
    "    concr_list.append(concr_lev)\n",
    "    \n",
    "    ##AoA\n",
    "    new_text = [''.join(char for char in s if char not in string.punctuation) for s in new_text]\n",
    "    new_text = [s for s in new_text if s]\n",
    "    for word in new_text:     \n",
    "        n_word_inlistlemmas += 1\n",
    "        word_is_present = False\n",
    "        for key, value in zip(AoA[\"words\"],AoA[\"values\"]):\n",
    "            if word == key:\n",
    "                word_is_present = True\n",
    "                AoA_score += value\n",
    "        if word_is_present == False:\n",
    "            non_present_word += 1\n",
    "        else:\n",
    "            present_word += 1\n",
    "    AoA_mean = AoA_score/present_word if present_word != 0 else 0\n",
    "    percentage_non_present_words = (non_present_word/n_word_inlistlemmas)*100 if n_word_inlistlemmas != 0 else 0\n",
    "    \n",
    "    #append AOA\n",
    "    AoA_list.append(AoA_mean)     \n",
    "df[\"AoA\"] = AoA_list\n",
    "df[\"Concreteness\"] = concr_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b3ec1c-fc58-422c-80cb-731adee60e24",
   "metadata": {},
   "source": [
    "## Adding Monitoring of IMPAQTS (METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a78edc-b6d2-40f7-b88a-8ea3157451fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon = pd.read_csv(\"Monitoring.csv\", encoding=\"latin-1\") #Cominetti et Al, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3039368-4124-4f0a-8c0c-4f1d5259a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append metadata\n",
    "data_en = []\n",
    "sottocorp= []\n",
    "sit_pol= []\n",
    "channel= []\n",
    "public= []\n",
    "orient = []\n",
    "for x in df[\"Filename\"]:\n",
    "    i = 0\n",
    "    for y,a,b,c,d,e,f in zip(mon[\"NOME\"],mon[\"DATA ENUNCIAZIONE\"], mon[\"SOTTOCORPUS TEMPORALE\"],mon[\"FORZA POLITICA\"],mon[\"DECADE\"],mon[\"TIPO\"],mon[\"ORIENTAMENTO\"]):\n",
    "        if x == y:\n",
    "            i += 1\n",
    "            data_en.append(a) \n",
    "            sottocorp.append(b)\n",
    "            sit_pol.append(c)\n",
    "            channel.append(d)\n",
    "            public.append(e)\n",
    "            orient.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a20680-8155-4eb8-a06a-4a81742f00b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = data_en\n",
    "df[\"subcorpus\"] = sottocorp\n",
    "df[\"Political_Orientation\"] = orient\n",
    "df[\"forza_politica\"] = sit_pol\n",
    "df[\"decade\"] = channel\n",
    "df[\"type\"] = public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b857c3-6081-4d25-ab26-e4a795838db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating type in a readable way\n",
    "new_tipo = []\n",
    "for x in df[\"Filename\"]:\n",
    "    x = str(x)\n",
    "    x = x[-2:]\n",
    "    if \"A\" in x:\n",
    "        x = \"Assemlea\"\n",
    "    if \"C\" in x:\n",
    "        x = \"Comizio\"\n",
    "    if \"P\" in x:\n",
    "        x = \"Riunione_Partito\"\n",
    "    if \"T\" in x:\n",
    "        x = \"Dichiarazione_Trasmessa\"\n",
    "    if \"F\" in x:\n",
    "        x = \"Dichiarazione_faccia_a_faccia\"\n",
    "    if \"N\" in x:\n",
    "        x = \"Dichiarazione_nuovi_media\"\n",
    "    if \"O\" in x:\n",
    "        x = \"Riunione_operativa\"\n",
    "    if \"I\" in x:\n",
    "        x = \"Intervista\"\n",
    "    \n",
    "    new_tipo.append(x)\n",
    "df[\"type\"] = new_tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3380a-c92d-45f9-b522-9da7b2df2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify populist parties (given classification in PopuList 3.0 (2023))\n",
    "so_called_populists = [\"FdI\", \"LN\", \"M5S\", \"IdV\", \"La Rete\", \"RC\", \"FI\", \"PdL\"]\n",
    "popll = []\n",
    "for x in df[\"forza_politica\"]:\n",
    "    m = 0\n",
    "    if x in so_called_populists:\n",
    "        m = 1\n",
    "    else:\n",
    "        m = 0\n",
    "    popll.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54435fe9-3939-40ea-93f3-ca53eb20c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_populist\"] = popll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6448036-b177-4633-be84-98105997b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Frequency\n",
    "docs = [nlp(text) for text in df[\"Text\"] ]\n",
    "filtered_tokens_per_doc = [[token.text.lower() for token in doc if token.pos_ in [\"NOUN\", \"VERB\"]] for doc in docs]\n",
    "# Define the path to your text file\n",
    "file_path = 'it_50k.txt'\n",
    "\n",
    "freq = pd.read_csv(file_path, sep=\" \", header=None, names=['Word', 'Frequency'])\n",
    "freq['Rank'] = freq['Frequency'].rank(method='dense', ascending=False).astype(int) #ranking words for frequency\n",
    "freq['Decile'] = pd.qcut(freq['Rank'], q=10, labels=False, duplicates='drop') + 1 #assigning decile to ranks (10 + out-of-vocabulary 11)\n",
    "freq[\"weight\"] = [x / 10 for x in freq['Decile']] #assigning a weight to smooth decile importance (especially the 11th)\n",
    "\n",
    "## calculate\n",
    "freq_dict = dict(zip(freq[\"Word\"], zip(freq[\"Decile\"], freq[\"weight\"])))\n",
    "\n",
    "df_mean_score = []\n",
    "non_present_words_list = []\n",
    "adjusted_non_present_score_list = []\n",
    "\n",
    "for doc in filtered_tokens_per_doc:\n",
    "    total_score, non_present_words = 0, 0\n",
    "    \n",
    "    for word in doc:\n",
    "        if word in freq_dict:\n",
    "            decile, weight = freq_dict[word]\n",
    "            score = decile * weight\n",
    "        else:\n",
    "            non_present_words += 1\n",
    "    \n",
    "    # Calculate the percentage of non-present words\n",
    "    n_tokens = len(doc) if doc else 1  # Prevent division by zero if doc is empty\n",
    "    non_present_percentage = (non_present_words / n_tokens * 100)\n",
    "    \n",
    "    # Adjust the score for non-present words based on their percentage\n",
    "    adjusted_non_present_score = 11 * (0.1 * (non_present_percentage / 100)) #non present words get 11th percentile but the lowest weight, multiplied for how many they are (the more unseen words, the more they count)\n",
    "    \n",
    "    # Recalculate the total score with adjusted score for non-present words\n",
    "    for word in doc:\n",
    "        if word in freq_dict:\n",
    "            decile, weight = freq_dict[word]\n",
    "            score = decile * weight\n",
    "        else:\n",
    "            score = adjusted_non_present_score  # Use the adjusted score for non-present words\n",
    "        total_score += score\n",
    "    \n",
    "    mean_score = total_score / n_tokens\n",
    "    adjusted_non_present_score_list.append(adjusted_non_present_score)\n",
    "    \n",
    "    df_mean_score.append(mean_score)\n",
    "    non_present_words_list.append(non_present_percentage)\n",
    "df[\"word_frequency\"] = df_mean_score #the higer the score, the LESS frequent a word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bb5f2-5e14-438b-97bb-bbce4ecf2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate optimal Topic x Decade (fixing decade)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "italian_stop_words = stopwords.words('italian')\n",
    "\n",
    "filtered_tokens_per_doc = [[token.text.lower() for token in doc if token.pos_ in [\"NOUN\"]] for doc in docs]\n",
    "filtered_tokens_per_doc = [' '.join(token) for token in filtered_tokens_per_doc]\n",
    "\n",
    "# Adjusting the decade column\n",
    "df['decade'] = df['decade'].replace({'2020-2022': '2020-2023'})\n",
    "\n",
    "# Ensure correct order of decades\n",
    "decade_order = ['1990-1999', '2000-2009', '2010-2019', '2020-2023']\n",
    "\n",
    "# Order for political orientation\n",
    "orientation_order = ['S', 'CS', 'I', 'C', 'CD', 'D']\n",
    "\n",
    "\n",
    "# Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=italian_stop_words, max_features=10000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"Text\"])\n",
    "\n",
    "# Finding the optimal number of clusters using the elbow method\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "for i in range(1, 50):  # Let's try from 1 to 10 clusters\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the results\n",
    "plt.plot(range(1, 50), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')  # Within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dcbf4-ade5-46fe-9a8e-4803828e0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a sparse matrix. To convert it to a dense format and put it in a DataFrame:\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# If you wish to include the document identifiers or any specific indexing, adjust accordingly:\n",
    "tfidf_df.index = df.index\n",
    "\n",
    "# Number of top words to retrieve\n",
    "top_n = 10  \n",
    "\n",
    "# Initialize an empty list to store the lists of important words for all documents\n",
    "important_words_all_docs = []\n",
    "\n",
    "# Iterate through each row/document in tfidf_df to find the top N important words\n",
    "for index, row in tfidf_df.iterrows():\n",
    "    # Sort the row to get the indices (words) of the top N values (TF-IDF scores)\n",
    "    top_words_indices = row.sort_values(ascending=False).head(top_n).index\n",
    "    # Convert indices to a list and append to the main list\n",
    "    important_words_all_docs.append(list(top_words_indices))\n",
    "\n",
    "#print(important_words_all_docs)\n",
    "df[\"tfidfwords\"] = important_words_all_docs\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Adjusting the decade column\n",
    "df['decade'] = df['decade'].replace({'2020-2022': '2020-2023'})\n",
    "\n",
    "# Ensure correct order of decades\n",
    "decade_order = ['1990-1999', '2000-2009', '2010-2019', '2020-2023']\n",
    "\n",
    "# Order for political orientation\n",
    "orientation_order = ['S', 'CS', 'I', 'C', 'CD', 'D']\n",
    "\n",
    "# No filtering step based on party document counts\n",
    "\n",
    "# Create a unique color for each party\n",
    "unique_parties = df['forza_politica'].unique()\n",
    "party_colors_hex = {party: plt.cm.tab20(i / len(unique_parties)) for i, party in enumerate(unique_parties)}\n",
    "\n",
    "# Convert RGBA to HEX\n",
    "party_colors_hex = {party: '#%02x%02x%02x' % (int(rgba[0]*255), int(rgba[1]*255), int(rgba[2]*255)) for party, rgba in party_colors_hex.items()}\n",
    "\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    party = kwargs['party']\n",
    "    return party_colors_hex.get(party, 'black')  # Default to black if party not found\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=len(orientation_order), ncols=len(decade_order), figsize=(20, 15), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "# Ensure axes is a 2D array\n",
    "if len(orientation_order) == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for (party, decade, orientation), words in df.groupby(['forza_politica', 'decade', 'Political_Orientation'])['tfidfwords'].sum().items():\n",
    "    row = orientation_order.index(orientation)\n",
    "    col = decade_order.index(decade)\n",
    "    ax = axes[row, col] if axes.ndim > 1 else axes[col]  # Support for 1D array of axes\n",
    "    \n",
    "    # Create a singular bag-of-words string\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(background_color='white', width=400, height=300,\n",
    "                          color_func=lambda *args, **kwargs: color_func(*args, **kwargs, party=party)).generate(text)\n",
    "    \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(f\"{party}\\n{decade} - {orientation}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d7301-9cf0-408d-af5b-0c4bdf913885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your DataFrame with 'decade' and 'text' columns\n",
    "# nltk.download('stopwords')\n",
    "italian_stop_words = stopwords.words('italian')\n",
    "\n",
    "# Preprocess the text: tokenize and remove stopwords\n",
    "def preprocess(text):\n",
    "    return [word for word in simple_preprocess(text, deacc=True) if word not in italian_stop_words]\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed'] = filtered_tokens_per_doc\n",
    "df['processed'] = df['processed'].map(preprocess)\n",
    "\n",
    "# Determine the optimal number of topics for each decade\n",
    "decades = df['decade'].unique()\n",
    "coherence_values = {}\n",
    "for decade in decades:\n",
    "    texts = df[df['decade'] == decade]['processed'].tolist()\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    coherence_scores = []\n",
    "    for num_topics in range(2, 11):  # Let's try from 2 to 10 topics\n",
    "        lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_lda)\n",
    "    \n",
    "    coherence_values[decade] = coherence_scores\n",
    "\n",
    "# Plotting the coherence scores\n",
    "plt.figure(figsize=(15, 7))\n",
    "for decade, scores in coherence_values.items():\n",
    "    plt.plot(range(2, 11), scores, marker='o', label=f'{decade}')\n",
    "plt.title('LDA Topic Coherence')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b5a36-a2ef-4f1b-a2d7-2824b95765d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Since documents are already preprocessed and tokenized\n",
    "def preprocess_document(tokens):\n",
    "    return [word for word in tokens if word not in italian_stop_words and word.isalpha()]\n",
    "\n",
    "\n",
    "# Group documents by decade and preprocess\n",
    "grouped_docs = df.groupby('decade')['processed'].apply(lambda docs: [preprocess_document(tokens) for tokens in docs]).to_dict()\n",
    "\n",
    "\n",
    "# Perform LDA for each decade\n",
    "lda_models = {}\n",
    "for decade, docs in grouped_docs.items():\n",
    "    # Create a dictionary and corpus\n",
    "    dictionary = Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    \n",
    "    # Number of topics for this decade\n",
    "    num_topics = optimal_topics[decade]\n",
    "    \n",
    "    # LDA Model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10)\n",
    "    lda_models[decade] = lda_model\n",
    "    \n",
    "    # Print the topics\n",
    "    print(f\"\\nTopics for {decade}:\")\n",
    "    for idx, topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "        print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af9c62-7e11-43b5-9583-4b5e1f5c0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the dominant topic for each document\n",
    "dominant_topics = []\n",
    "\n",
    "# Iterate over each document in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    decade = row['decade']\n",
    "    processed_doc = row['processed']\n",
    "    \n",
    "    # Get the appropriate LDA model and dictionary for the document's decade\n",
    "    lda_model = lda_models[decade]\n",
    "    dictionary = lda_model.id2word\n",
    "    \n",
    "    # Convert the processed document to BOW format\n",
    "    bow = dictionary.doc2bow(processed_doc)\n",
    "    \n",
    "    # Get the topic distribution for the document\n",
    "    doc_distribution = lda_model.get_document_topics(bow)\n",
    "    \n",
    "    # Sort the topics by probability to find the dominant one\n",
    "    doc_distribution.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take the most probable topic for the document\n",
    "    dominant_topic, prob = doc_distribution[0] if doc_distribution else (None, 0)\n",
    "    \n",
    "    # Generate a unique name for the dominant topic incorporating the decade\n",
    "    unique_topic_name = f\"{decade}_Topic{dominant_topic}\" if dominant_topic is not None else \"No_Dominant_Topic\"\n",
    "    \n",
    "    # Append the unique topic name to our list\n",
    "    dominant_topics.append(unique_topic_name)\n",
    "\n",
    "# Assign the dominant topic names to a new column in the DataFrame\n",
    "df['Topic'] = dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a01f2-9da7-4c9e-871c-15ecd5e9f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating parties in majority or opposition in parliament\n",
    "cal = {\"gennaio\":1, \"febbraio\":2,\"marzo\":3,\"aprile\":4,\"maggio\":5,\"giugno\":6,\"luglio\":7,\"agosto\":8,\"settembre\":9,\"ottobre\":10,\"novembre\":11,\"dicembre\":12,}\n",
    "mag_opp = pd.read_csv('maggioranzainparlamento.csv', encoding='latin-1')\n",
    "inzio = []\n",
    "fin = []\n",
    "interval = []\n",
    "i = 0\n",
    "for x, y in zip(mag_opp[\"Inizio\"],mag_opp[\"Fine\"]):\n",
    "    i+= 1\n",
    "    x = x.split()\n",
    "    if i == 68:\n",
    "        y = \"26 giugno 2023\"\n",
    "    y = y.split()\n",
    "    for m in cal:\n",
    "        if m == x[1]:\n",
    "            g = cal[m]\n",
    "        if m == y[1]:\n",
    "            f = cal[m]\n",
    "    xx = str(x[0]) + \"/\" + str(g) + \"/\" + str(x[2]) \n",
    "    yy = str(y[0]) + \"/\" + str(f) + \"/\" + str(y[2])\n",
    "    inzio.append(xx)\n",
    "    fin.append(yy)\n",
    "    interval.append(pd.date_range(start=xx, end=yy))\n",
    "\n",
    "mag_opp[\"Inizio\"] = inzio #range\n",
    "mag_opp[\"Fine\"] = fin #range\n",
    "\n",
    "majority = []\n",
    "for i, (x, pp, decade, filename) in enumerate(zip(df[\"date\"], df[\"forza_politica\"], df[\"decade\"], df[\"Filename\"])):\n",
    "    # Initialize maop as 0 at the start of each iteration\n",
    "    maop = 0\n",
    "\n",
    "    if pd.isnull(x):\n",
    "        # Derive 'x' from 'filename' by removing the last 3 characters and taking the last 2 digits\n",
    "        try:\n",
    "            x = int(filename[:-3][-2:])\n",
    "        except ValueError:\n",
    "            # Log error and continue to next iteration after appending maop (which is 0)\n",
    "            print(f\"Invalid filename info at index {i}: filename={filename}, derived value attempted: {filename[:-3][-2:]}\")\n",
    "            majority.append(maop)  # Ensure a value is appended for this row\n",
    "            continue\n",
    "    else:\n",
    "        # Extract year from 'x' when it is a date string\n",
    "        try:\n",
    "            x = int(pd.to_datetime(x).year)\n",
    "        except ValueError:\n",
    "            # Log error and continue to next iteration after appending maop (which is 0)\n",
    "            print(f\"Invalid date format at index {i}: date={x}, decade={decade}, forza_politica={pp}\")\n",
    "            majority.append(maop)  # Ensure a value is appended for this row\n",
    "            continue\n",
    "\n",
    "    # Proceed with existing logic\n",
    "    for t, mag in zip(interval, mag_opp[\"Partito\"]):\n",
    "        years_range = t.year.unique()\n",
    "        if x in years_range:\n",
    "            parties = mag.split('-')\n",
    "            if pp.strip() in [p.strip() for p in parties]:\n",
    "                maop = 1\n",
    "                break  # Found a matching condition, no need to check further\n",
    "\n",
    "    # Append the maop value after processing the current row\n",
    "    majority.append(maop)\n",
    "\n",
    "# Check if the lengths match as expected\n",
    "#print(f\"Length of 'majority': {len(majority)}, Expected: {len(df)}\")\n",
    "df[\"is_majority\"] = majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6587638-0830-4b82-9413-1675da676dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"offical_text_profiling_impaqts.csv\", header=True, index=False, encoding=\"UTF-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
