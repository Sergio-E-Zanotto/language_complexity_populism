---
title: "R Notebook"
output: html_notebook
---

```{r, include = F}
library(tidyverse)
library(glmnet)
library(car)
library(lme4)
library(caret)
library(MASS)
library(brms)
library(loo)
library(performance)
```


```{r, include = F}
data <- read_csv('offical_text_profiling_impaqts.csv')
```



```{r}
data <- data[data$author %in% names(which(table(data$author) > 7)), ]
```

```{r}
table(as.factor(data$is_populist))
```


```{r}
#data$decade<- as.factor(ifelse(data$decade %in% c("1990-1999", "2000-2009"), "1990-2009", as.character(data$decade)))
table(as.factor(data$decade))
```


```{r}
df <- data
# Define columns not to scale
cols_not_to_scale <- c("interruption", "author", "type", "decade", "is_majority", "is_populist", "date", "subcorpus", "Political_Orientation", "forza_politica", "document_embedding", "Filename", "relevant_words", "topic", "transcriber", "topic_numeric", "topic_scaled")


# scale all other numeric columns
df <- df %>%
  mutate(across(setdiff(names(df), cols_not_to_scale), scale))


```

```{r}
# Convert topics to a factor ensuring the order reflects from first to last topic
df$topic <- factor(df$topic, levels = unique(df$topic))

# Convert the factor to numeric values
df$topic_numeric <- as.numeric(df$topic) - 1 # Now it ranges from 0 to (number of unique topics - 1)

# Scale these values to range from 0 to 1
df$topic_scaled <- (df$topic_numeric - min(df$topic_numeric)) / (max(df$topic_numeric) - min(df$topic_numeric))

# View the changes
df$topic <- df$topic_scaled

```

```{r}
# step2 #check variance to avoid mistakes

cols_not_to_scale <- c("interruption","author", "type", "decade", "is_majority", "is_populist", "date", "subcorpus", "Political_Orientation", "forza_politica", "document_embedding", "Filename", "relevant_words", "topic", "transcriber", "topic_numeric",  "topic_scaled","decade_numeric",  "decade_scaled")


all_columns <- colnames(df)

# Step 1: Exclude non-predictor columns
predictor_columns <- setdiff(all_columns, cols_not_to_scale)


# Prepare the final formula and model matrix
final_predictor_formula <- paste(lapply(predictor_columns, function(col) paste0("`", col, "`")), collapse = " + ")
final_full_formula <- as.formula(paste("is_populist ~", final_predictor_formula))
X <- model.matrix(final_full_formula, data = df)
Y <- df$is_populist
```


```{r}
# step 1) literature GLM
model1 <- glm(is_populist ~ G_index + char_per_tok + Type_token_ratio + lexical_density + word_rarity + avg_max_depth 
                + decade + type + topic + is_majority,
                data= df, family = binomial(link="logit")
              )
summary(model1)
```



```{r}
# step 1) literature GLM
model_literature_features <- glmer(is_populist ~ G_index + char_per_tok + Type_token_ratio + lexical_density + word_rarity + avg_max_depth 
                + decade + type + topic + is_majority + (1|author) + (1|transcriber),
                data= df, family = binomial(link="logit"), control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 50000))
              )
summary(model_literature_features)
```
```{r}
vif(model1)
```





```{r}
cv.lambda <- cv.glmnet(x=X, y=as.factor(Y), 
                        alpha = 0,
                       family = binomial
                        )  
   
  plot(cv.lambda)  
```

```{r}
cv.lambda$lambda.min 
```

```{r, fig.height=10}
#ridge path
  plot(cv.lambda$glmnet.fit, 
       "lambda", label=FALSE)
```






```{r}
lmin        <- cv.lambda$lambda.min
ridge.model <- glmnet(x=X, y=Y,
                        alpha = 0, 
                      family = binomial,
                        lambda = lmin)
```

```{r}
# Convert the sparse matrix of coefficients to a regular vector, excluding the intercept
coefficients <- as.vector(ridge.model$beta[-1,])

# Get the names of the coefficients
feature_names <- rownames(ridge.model$beta)[-1]

# Create a named vector of coefficients
named_coefficients <- setNames(coefficients, feature_names)

# Sort the coefficients by their absolute values in descending order
sorted_indices <- order(abs(named_coefficients), decreasing = TRUE)
sorted_coefficients <- named_coefficients[sorted_indices]

# Extract the top 10 features
top_15_features <- head(sorted_coefficients, 15)
top_5_features <- head(sorted_coefficients, 5)

# Print the top 10 features and their coefficients
print(top_5_features)

```

```{r}
# Convert the names of the top 10 features to a formula-friendly format
predictors_formula <- paste(names(top_15_features), collapse=" + ")

# Construct the GLMM formula including the top 10 features and a random intercept for 'author'
formula <- as.formula(paste("is_populist ~ ", predictors_formula, " + decade + type + topic + is_majority" #+ (1|author) + (1|transcriber)
                            ))

# Fit the GLMM model using the constructed formula
glmer_model <- glm(formula, data = df, family = binomial(link="logit")#, control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 50000))
                   )

# Check the summary of the model
summary(glmer_model)

```



```{r}
vif(glmer_model)
```


```{r}
# Convert the names of the top 10 features to a formula-friendly format
predictors_formula <- paste(names(top_15_features), collapse=" + ")

# Construct the GLMM formula including the top 10 features and a random intercept for 'author'
formula <- as.formula(paste("is_populist ~ ", predictors_formula, "+ decade + type + topic + is_majority + (1|author) + (1|transcriber)"
                            ))

# Fit the GLMM model using the constructed formula
model_selected_features <- glmer(formula, data = df, family = binomial(link="logit"), control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 50000))
                   )

# Check the summary of the model
summary(model_selected_features)
```
```{r}
vif(glmer_model)
```






```{r}
anova(model_literature_features, model_selected_features)
```




```{r}
mean_features <- data %>%
  group_by(is_populist) %>%
  summarize(
    Mean_G_index = mean(G_index, na.rm = TRUE),
    Mean_char_per_tok = mean(char_per_tok, na.rm = TRUE),
    Mean_Type_token_ratio = mean(Type_token_ratio, na.rm = TRUE),
    Mean_lexical_density = mean(lexical_density, na.rm = TRUE),
    Mean_word_rarity = mean(word_rarity, na.rm = TRUE),
    Mean_avg_max_depth = mean(avg_max_depth, na.rm = TRUE),
    Mean_upos_dist_PROPN = mean(upos_dist_PROPN, na.rm = TRUE),
    Mean_dep_dist_det_predet = mean(`dep_dist_det:predet`, na.rm = TRUE),
    Mean_Populist_words_ratio = mean(Populist_words_ratio, na.rm = TRUE),
    Mean_verbs_num_pers_dist_Sing2 = mean(`verbs_num_pers_dist_Sing+2`, na.rm = TRUE),
    Mean_verbal_root_perc = mean(verbal_root_perc, na.rm = TRUE),
    Mean_verbs_mood_dist_Cnd = mean(verbs_mood_dist_Cnd, na.rm = TRUE),
    Mean_verbs_form_dist_Fin = mean(verbs_form_dist_Fin, na.rm = TRUE),
    Mean_upos_dist_ADJ = mean(upos_dist_ADJ, na.rm = TRUE),
    Mean_subordinate_dist_4 = mean(subordinate_dist_4, na.rm = TRUE),
    Mean_verb_edges_dist_1 = mean(verb_edges_dist_1, na.rm = TRUE),
  )

# Print the results
print(mean_features)

```


```{r}
# Assuming 'mean_features' is your tibble
# Subtract the values in the second row from the first row for each column
differences <- mean_features[2, -1] - mean_features[1, -1]

# To make it a tibble again and more readable, you can use the names of the first row
differences_tibble <- tibble::tibble(Column = names(differences), Difference = as.numeric(differences))

# Print the differences tibble
print(differences_tibble)

```









